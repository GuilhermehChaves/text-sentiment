# -*- coding: utf-8 -*-
"""pln.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FvafDIK-GRLAu0D_7Hm-fT5fuAZ-_7KY
"""

import numpy as np
import math
import re
import pandas as pd
from bs4 import BeautifulSoup
from google.colab import drive
import zipfile
import seaborn as sns
import spacy as sp
import string
import random
import matplotlib.pyplot as plt

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 2.x
import tensorflow as tf
tf.__version__

from tensorflow.keras import layers
import tensorflow_datasets as tfds

drive.mount("/content/drive")

path = '/content/drive/My Drive/datasets/original.zip'
zip_object = zipfile.ZipFile(file = path, mode = 'r')
zip_object.extractall('./')
zip_object.close()

cols = ['sentiment', 'id', 'date', 'query', 'user', 'text']

train_data = pd.read_csv('/content/trainingandtestdata/train.csv', 
                         header = None, 
                         names = cols, 
                         engine = 'python',
                         encoding = 'latin1')

test_data = pd.read_csv('/content/trainingandtestdata/test.csv', 
                         header = None, 
                         names = cols, 
                         engine = 'python',
                         encoding = 'latin1')

data = train_data

data.drop(['id', 'date', 'query', 'user'], axis = 1, inplace = True)

x = data.iloc[:, 1].values

y = data.iloc[:, 0].values

from sklearn.model_selection import train_test_split
x, _, y, _ = train_test_split(x,y, test_size = 0.9950, stratify = y)

unique, counts = np.unique(y, return_counts = True)

nlp = sp.load('en')

stop_words = sp.lang.en.STOP_WORDS

def clean_tweets(tweet):
  tweet = BeautifulSoup(tweet, 'lxml').get_text()
  tweet = re.sub(r"@[A-Za-z0-9]+", ' ', tweet)
  tweet = re.sub(r"https?://[A-Za-z0-9./]+", ' ', tweet)
  tweet = re.sub(r"[^a-zA-Z.!?]", ' ', tweet)
  tweet = re.sub(r" +", " ", tweet)

  tweet = tweet.lower()
  document = nlp(tweet)

  words = []
  for token in document:
    words.append(token.text)
  
  words = [word for word in words if word not in stop_words and word not in string.punctuation]
  words = ' '.join([str(element) for element in words])

  return words

data_clean = [clean_tweets(tweet) for tweet in x]

data_labels = y

data_labels[data_labels == 4] = 1

tokenizer = tfds.features.text.SubwordTextEncoder.build_from_corpus(data_clean, target_vocab_size=2**16)

data_inputs = [tokenizer.encode(sentence) for sentence in data_clean]

max_length = max([len(sentence) for sentence in data_inputs])

data_inputs = tf.keras.preprocessing.sequence.pad_sequences(data_inputs, value=0, padding="post", maxlen = max_length)

train_inputs, test_inputs, train_labels, test_labels = train_test_split(data_inputs, 
                                                                        data_labels,
                                                                        test_size = 0.1,
                                                                        stratify = data_labels)

"""## Modelo"""

class DCNN(tf.keras.Model):

  def __init__(self,
               vocab_size,
               emb_dim=128,
               nb_filters=50,
               ffn_units=512,
               nb_classes=2,
               dropout_rate=0.1,
               training=True,
               name="dcnn"):
    super(DCNN, self).__init__(name=name)

    self.embedding = layers.Embedding(vocab_size, emb_dim)

    self.bigram = layers.Conv1D(filters=nb_filters, kernel_size=2, padding='same', activation='relu')

    self.trigram = layers.Conv1D(filters=nb_filters, kernel_size=3, padding='same', activation='relu')

    self.fourgram = layers.Conv1D(filters=nb_filters, kernel_size=4, padding='same', activation='relu')

    self.pool = layers.GlobalMaxPool1D()

    self.dense_1 = layers.Dense(units = ffn_units, activation = 'relu')
    self.dropout = layers.Dropout(rate = dropout_rate)
    if nb_classes == 2:
      self.last_dense = layers.Dense(units = 1, activation = 'sigmoid')
    else:
      self.last_dense = layers.Dense(units = nb_classes, activation = 'softmax')

  def call(self, inputs, training):
    x = self.embedding(inputs)

    x_1 = self.bigram(x)
    x_1 = self.pool(x_1)

    x_2 = self.trigram(x)
    x_2 = self.pool(x_2)

    x_3 = self.fourgram(x)
    x_3 = self.pool(x_3)

    merged = tf.concat([x_1, x_2, x_3], axis = -1)
    merged = self.dense_1(merged)
    merged = self.dropout(merged, training)
    output = self.last_dense(merged)

    return output

"""## Treinamento"""

vocab_size = tokenizer.vocab_size

emb_dim = 200
nb_filters = 100
ffn_units = 256
nb_classes = len(set(train_labels))

dropout_rate = 0.2
nb_epochs = 10
batch_size = 64

Dcnn = DCNN(vocab_size = vocab_size,
            emb_dim = emb_dim,
            nb_filters = nb_filters,
            ffn_units = ffn_units,
            nb_classes = nb_classes,
            dropout_rate = dropout_rate
            )

if nb_classes == 2:
  Dcnn.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])
else:
  Dcnn.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])

checkpoint_path = "./"
checkpoint = tf.train.Checkpoint(Dcnn=Dcnn)
checkpoint_manager = tf.train.CheckpointManager(checkpoint, checkpoint_path, max_to_keep = 5)

if checkpoint_manager.latest_checkpoint:
  checkpoint.restore(checkpoint_manager.latest_checkpoint)
  print('Lates checkpoint restored')

history = Dcnn.fit(train_inputs, train_labels,
                   batch_size = batch_size,
                   epochs = nb_epochs,
                   verbose = 1,
                   validation_split = 0.1)

checkpoint_manager.save()

results = Dcnn.evaluate(test_inputs, test_labels, batch_size = batch_size)

y_pred_test = Dcnn.predict(test_inputs)

y_pred_test = (y_pred_test > 0.5)

from sklearn.metrics import confusion_matrix
cm = confusion_matrix(test_labels, y_pred_test)

print(cm)

sns.heatmap(cm, annot =True)

"""## Gráficos"""

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model loss progress during training and validation')
plt.xlabel('Epoch')
plt.ylabel('Lossess')
plt.legend(['training_loss', 'Validating loss'])

plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model accurary progress during training and validation')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend(['training_accuracy', 'Validating accuracy'])

"""## Previsões"""

text = tokenizer.encode('Hi I love you man')
text

predict = Dcnn(np.array([text]), training = False).numpy()
print("Good" if predict > 0.5 else "Bad")